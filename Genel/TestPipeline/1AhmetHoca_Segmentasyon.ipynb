{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydicom opencv-python-headless[app] nibabel matplotlib albumentations tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import jaccard_score\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "height,width = (256,256) \n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.down_sample = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = self.double_conv(x)\n",
    "        down_out = self.down_sample(skip_out)\n",
    "        return (down_out, skip_out)\n",
    "\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlock, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, down_input, skip_input):\n",
    "        x = self.up_sample(down_input)\n",
    "        x = torch.cat([x, skip_input], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_classes=3, up_sample_mode='conv_transpose'):\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        # Downsampling Path\n",
    "        self.down_conv1 = DownBlock(3, 64)\n",
    "        self.down_conv2 = DownBlock(64, 128)\n",
    "        self.down_conv3 = DownBlock(128, 256)\n",
    "        self.down_conv4 = DownBlock(256, 512)\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024)\n",
    "        # Upsampling Path\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
    "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final Convolution\n",
    "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1_out = self.down_conv1(x)\n",
    "        x, skip2_out = self.down_conv2(x)\n",
    "        x, skip3_out = self.down_conv3(x)\n",
    "        x, skip4_out = self.down_conv4(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.up_conv4(x, skip4_out)\n",
    "        x = self.up_conv3(x, skip3_out)\n",
    "        x = self.up_conv2(x, skip2_out)\n",
    "        x = self.up_conv1(x, skip1_out)\n",
    "        x = self.conv_last(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Get UNet model\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Temp:\n",
    "    _temp_img=None\n",
    "    _temp_mock=None\n",
    "    rel_vals=None\n",
    "from skimage import measure\n",
    "\n",
    "def get_largest_mask(mask):\n",
    "    labels_mask = measure.label(mask)\n",
    "    regions = measure.regionprops(labels_mask)\n",
    "    regions.sort(key=lambda x: x.area, reverse=True)\n",
    "    if len(regions) > 1:\n",
    "        for rg in regions[1:]:\n",
    "            labels_mask[rg.coords[:,0], rg.coords[:,1]] = 0\n",
    "    labels_mask[labels_mask!=0] = 1\n",
    "    mask = labels_mask\n",
    "    return mask \n",
    "def crop_and_mask_dcm_image(masks, real_img):\n",
    "    real_w,real_h=real_img.shape\n",
    "    mock_masks=cv2.resize(masks,(real_h,real_w), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    Temp._temp_mock=mock_masks\n",
    "    mock_masks=get_largest_mask(mock_masks)\n",
    "    rslt = mock_masks* real_img\n",
    "\n",
    "    rows, cols = np.where(mock_masks!= 0)\n",
    "    y1, x1 = np.min(rows), np.min(cols)\n",
    "    y2, x2 = np.max(rows), np.max(cols)\n",
    "    # y1_rel,y2_rel=y1/256,y2/256\n",
    "    # x1_rel,x2_rel=x1/256,x2/256\n",
    "    # Temp.rel_vals=(y1_rel,y2_rel,x1_rel,x2_rel)\n",
    "    # print(y1,y2)\n",
    "    # print(x1,x2)\n",
    "\n",
    "\n",
    "    # Görüntüyü bounding box ile kırp\n",
    "    result = rslt[y1:y2+1, x1:x2+1]\n",
    "\n",
    "    # Maske dışındaki alanı siyah renge dök\n",
    "    result[mock_masks[y1:y2+1, x1:x2+1] == 0] = 0\n",
    "    \n",
    "    Temp._temp_img=result\n",
    "\n",
    "    return rslt, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "PATH=\"/home/uraninjo/Desktop/Çalışmalar/Comp/son500\"# TODO: Change this\n",
    "#PATH=\"/home/uraninjo/Desktop/Çalışmalar/Comp/son500\"\n",
    "test_images_path=sorted(glob.glob(f'{PATH}/*/*/*.dcm'))  # Görüntü yollarının listesi\n",
    "\n",
    "classes = [0, 1, 2]  # Orijinal etiket değerleri\n",
    "print(len(test_images_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.load_state_dict(torch.load(\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/checkpoints3/Kullanılacaklar/Segmentasyon_Zeynep2Kanal_6_ValLoss0.5481_diceScore0.9260.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "            A.Resize(height=256,width=256,p=1.0),\n",
    "            A.augmentations.transforms.CLAHE(clip_limit=(2.0,3.0), tile_grid_size=(8, 8), always_apply=False, p=0.5) \n",
    "        ])\n",
    "to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "try:\n",
    "    os.mkdir(f\"/home/uraninjo/Desktop/Çalışmalar/Comp/SegmentasyonOutput\")#{__hno}\n",
    "    # TODO: Change this\n",
    "except:\n",
    "    pass\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_images_path)):\n",
    "        try:\n",
    "            __tür=test_images_path[i].split(\"/\")[-1].split(\".\")[0]\n",
    "            __hno=test_images_path[i].split(\"/\")[-2]\n",
    "            img = pydicom.dcmread(test_images_path[i], force=True)\n",
    "            monochrome=img.PhotometricInterpretation\n",
    "            img=img.pixel_array\n",
    "            if monochrome=='MONOCHROME1':\n",
    "                img=np.invert(img)\n",
    "            img = ((img - img.min()) * (1/(img.max() - img.min()) * 255)).astype('uint8')\n",
    "            transformed = transform(image=img)\n",
    "            img_t = transformed[\"image\"]\n",
    "            img_t = np.array([img_t,img_t,img_t])\n",
    "\n",
    "            img_t=np.moveaxis(img_t,0,-1)\n",
    "            img_t = to_tensor(img_t)\n",
    "            img_t = torch.unsqueeze(img_t, 0)\n",
    "            inputs = img_t.to(device)\n",
    "            outputs = model(inputs)\n",
    "            output_np = outputs.detach().cpu().numpy()\n",
    "            output_np = output_np - np.min(output_np)\n",
    "            output_np = output_np / np.max(output_np)\n",
    "            predicted_masks = np.around(output_np, decimals=0, out=None)\n",
    "\n",
    "            mask_1 = predicted_masks[0][1,:,:]\n",
    "            mask_2 = predicted_masks[0][2,:,:]\n",
    "            combined_mask = np.maximum(mask_1, mask_2)\n",
    "\n",
    "            rslt, result = crop_and_mask_dcm_image(combined_mask,img)#inputs[0][0,:,:].cpu().numpy()\n",
    "            result_resize = cv2.resize(result.astype('uint8'),(250,500))\n",
    "            try:\n",
    "                os.mkdir(f\"/home/uraninjo/Desktop/Çalışmalar/Comp/SegmentasyonOutput/{__hno}\")#{__hno}\n",
    "                # TODO: Change this\n",
    "            except:\n",
    "                pass\n",
    "                # print(\"FoldAttributeError: 'FileDataset' object has no attribute 'PhotometricInterpretation'er already exists\")\n",
    "            plt.imsave(f\"/home/uraninjo/Desktop/Çalışmalar/Comp/SegmentasyonOutput/{__hno}/{__tür}.png\",result_resize,cmap=\"gray\")#{__hno}/{__tür}\n",
    "            # TODO: Change this\n",
    "            print(f\"Dosya oluşturuldu sıra:{i}-{__tür}-{__hno}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Hatalı\", test_images_path[i])\n",
    "            continue\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
