{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,roc_auc_score,roc_curve\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "EPOCH=20\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((500,250)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((500,250)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((500,250)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'C:/Users/yeni/Desktop/TEKNOFEST/ayrilmis'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                        data_transforms[x])\n",
    "                    for x in ['train', 'val','test']}\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=0),\n",
    "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=0),\n",
    "    'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=0)\n",
    "                                            }\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "dataset_sizes\n",
    "def train_model(model, criterion, optimizer, scheduler, name, num_epochs=25):\n",
    "\n",
    "    #Creating a folder to save the model performance.\n",
    "    try:\n",
    "        os.mkdir(f'modelPerformance/{name}')\n",
    "    except:\n",
    "        print('Dosya var')\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            #epochs\n",
    "            \n",
    "            epoch=int(len(image_datasets[phase])/BATCH_SIZE)\n",
    "            \n",
    "            for _ in tqdm(range(epoch)):\n",
    "                #Loading Data\n",
    "                \n",
    "                inputs, labels = next(iter(dataloaders[phase]))\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    labels = labels.to(device)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            \n",
    "            #epoch_auc= running_auc/(dataset_sizes[phase]-error)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            #AUC: {:.4f} , epoch_auc\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(        \n",
    "                phase, epoch_loss, epoch_acc))\n",
    "    \n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model,'modelPerformance/{}/best_model_{:.4f}acc_{}epochs.h5'.format(name,epoch_acc,num_epochs))\n",
    "\n",
    "                train_losses = []\n",
    "                valid_losses = []\n",
    "            \n",
    "        print()\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    \n",
    "    with open(f'modelPerformance/{name}/'+sorted(os.listdir(f'modelPerformance/{name}/'))[-1], 'rb') as f:\n",
    "        buffer = io.BytesIO(f.read())\n",
    "    model=torch.load(buffer)\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=(40,40))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosyalar var\n"
     ]
    }
   ],
   "source": [
    "# A dictionary of models.\n",
    "\n",
    "modeller={\n",
    "    'densenet121':models.densenet121(pretrained=True)\n",
    "}\n",
    "try:\n",
    "    os.mkdir('./modelPerformance')\n",
    "except:\n",
    "    print('Dosyalar var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya var\n",
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [32:16<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0700 Acc: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:50<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0660 Acc: 0.6719\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:47<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0659 Acc: 0.6748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:59<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0415 Acc: 0.6979\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [31:11<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0565 Acc: 0.6835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:53<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0872 Acc: 0.6514\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [30:56<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0588 Acc: 0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:39<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0548 Acc: 0.6860\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [30:42<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0612 Acc: 0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:52<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1014 Acc: 0.6369\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [28:34<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0688 Acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:36<00:00, 17.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1253 Acc: 0.6139\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:41<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0575 Acc: 0.6813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:47<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1291 Acc: 0.6052\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:57<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0569 Acc: 0.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:39<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0971 Acc: 0.6408\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:06<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0620 Acc: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:38<00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1124 Acc: 0.6284\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [33:25<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0549 Acc: 0.6846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [03:14<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1369 Acc: 0.6041\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [31:47<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0547 Acc: 0.6845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:55<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1164 Acc: 0.6202\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [30:14<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0563 Acc: 0.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:48<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0874 Acc: 0.6495\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:34<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0559 Acc: 0.6838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:46<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0989 Acc: 0.6367\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:55<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0565 Acc: 0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:46<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1001 Acc: 0.6345\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:49<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0531 Acc: 0.6866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:45<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1448 Acc: 0.5928\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10798/10798 [29:40<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0574 Acc: 0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2699/2699 [02:45<00:00, 16.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.0957 Acc: 0.6417\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 5205/10798 [14:39<15:45,  5.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m exp_lr_scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer_ft, step_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39m# TRAINING\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m     29\u001b[0m                         num_epochs\u001b[39m=\u001b[39;49mEPOCH)\n",
      "Cell \u001b[1;32mIn[33], line 83\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, name, num_epochs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m# track history if only in train\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 83\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     84\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m     86\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\densenet.py:214\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 214\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m    215\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(features, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    216\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madaptive_avg_pool2d(out, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\densenet.py:123\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    121\u001b[0m features \u001b[39m=\u001b[39m [init_features]\n\u001b[0;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 123\u001b[0m     new_features \u001b[39m=\u001b[39m layer(features)\n\u001b[0;32m    124\u001b[0m     features\u001b[39m.\u001b[39mappend(new_features)\n\u001b[0;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(features, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\densenet.py:91\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn_function(prev_features)\n\u001b[1;32m---> 91\u001b[0m new_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(bottleneck_output)))\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     93\u001b[0m     new_features \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(new_features, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yeni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name,model in modeller.items(): \n",
    "    #model_ft = model\" \n",
    "    model_ft=torch.load('C:/Users/yeni/Desktop/TEKNOFEST/Mergen1-Teknofest/ertan/modelPerformance/densenet121/best_model_0.6864acc_20epochs.h5')\n",
    "    \n",
    "    \"\"\"\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc =nn.Sequential(nn.Linear(num_ftrs, len(class_names)), nn.Softmax())\"\"\"\n",
    "\n",
    "    \"\"\" num_ftrs=model.heads[-1].in_features\n",
    "    model_ft.heads[-1]=nn.Sequential(nn.Linear(num_ftrs, len(class_names)), nn.Softmax())\"\"\"\n",
    "    \n",
    "    num_ftrs=models.densenet121(pretrained=True).classifier.in_features\n",
    "    model_ft.classifier =nn.Sequential(nn.Linear(num_ftrs, len(class_names)), nn.Softmax())\n",
    "    \n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "    \n",
    "    #optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.0001)\n",
    "\n",
    "    # TRAINING\n",
    "    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, name=name,\n",
    "                            num_epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class get_metric():\n",
    "\n",
    "    def get_accuracy_graph(epochs, train_acc, val_acc):  # draw validation and train accuracy graphs\n",
    "        plt.plot(epochs, train_acc, color='#006BA4')\n",
    "        plt.plot(epochs, val_acc, color='#FF800E')\n",
    "        plt.grid(b=True, which='major', color='lightgray')\n",
    "        plt.grid(b=True, which='minor', color='lightgray')\n",
    "        plt.xticks(np.arange(0, 45, 5))\n",
    "        plt.yticks(np.arange(0.5, 1, 0.05))\n",
    "        plt.rcParams['figure.figsize'] = (8, 6)\n",
    "        plt.rcParams['figure.dpi'] = 600\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "        plt.legend(['Training Acc.', 'Validation Acc.'], loc='lower right')\n",
    "        plt.show()\n",
    "\n",
    "    def get_loss_graph(epochs, train_losses, val_losses):  # draw validation and train loss graphs\n",
    "        matplotlib.rcdefaults()\n",
    "        plt.plot(epochs, train_losses, color='#006BA4')\n",
    "        plt.plot(epochs, val_losses, color='#FF800E')\n",
    "        plt.grid(b=True, which='major', color='lightgray')\n",
    "        plt.grid(b=True, which='minor', color='lightgray')\n",
    "        plt.xticks(np.arange(0, 45, 5))\n",
    "        plt.yticks(np.arange(0, 1.2, 0.2))\n",
    "        plt.rcParams['figure.dpi'] = 600\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss vs Validation Loss\")\n",
    "        plt.legend(['Training Loss', 'Validation Loss'], loc='lower right')\n",
    "        plt.show()\n",
    "\n",
    "    def test_label_predictions(model, device, test_loader):  # calculate outputs on test dataset for get metrics\n",
    "        model.eval()\n",
    "        actuals = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                model.to(device)\n",
    "                output = model(data)\n",
    "                prediction = output.argmax(dim=1, keepdim=True)\n",
    "                actuals.extend(target.view_as(prediction))\n",
    "                predictions.extend(prediction)\n",
    "        return [i.item() for i in actuals], [i.item() for i in predictions]\n",
    "    \n",
    "    def test_label_predictions_el2(model_0,model_1,model_2,model_3, device, test_loader):\n",
    "    \n",
    "        actuals = []\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                outputs_0 = model_0(data).cuda().cpu()\n",
    "                _, predicted_0 =torch.max(outputs_0.data, 1) \n",
    "\n",
    "                outputs_1 = model_1(data)\n",
    "                _, predicted_1 =torch.max(outputs_1.data, 1) \n",
    "\n",
    "                outputs_2 = model_2(data)\n",
    "                _, predicted_2 =torch.max(outputs_2.data, 1)\n",
    "\n",
    "                outputs_3 = model_3(data)\n",
    "                _, predicted_3 =torch.max(outputs_3.data, 1)\n",
    "\n",
    "                final_pred=predicted_1\n",
    "                size=final_pred.size()\n",
    "\n",
    "                for i in range(0,(size[0])):   \n",
    "                    a=0              \n",
    "                    if predicted_2[i].item()==0 and predicted_3[i].item()==0:\n",
    "\n",
    "                        if predicted_1[i].item()==1:\n",
    "                            final_pred[i]=1\n",
    "\n",
    "                        if predicted_1[i].item()==0:\n",
    "                            final_pred[i]=0\n",
    "                        a+=1\n",
    "\n",
    "                    if (predicted_0[i].item()==1 and predicted_1[i].item()==1) :\n",
    "\n",
    "                        a+=1   \n",
    "                        if predicted_3[i].item()==0:\n",
    "                            final_pred[i]=0 \n",
    "\n",
    "                        if predicted_3[i].item()!=0:\n",
    "                            final_pred[i]=1                    \n",
    "                    if a==0:                   \n",
    "                        final_pred[i]=predicted_2[i] \n",
    "                actuals.extend(target.view_as(final_pred))\n",
    "                predictions.extend(final_pred)\n",
    "        return [i.item() for i in actuals], [i.item() for i in predictions]\n",
    "\n",
    "    def test_model(model ,device, test_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print('Correct Prediction: {:d}  Total Images: {:d}'.format(correct, total))\n",
    "        print('Test Accuracy = {:f}'.format(correct / total))\n",
    "    \n",
    "    def test_model_el2(model_0,model_1,model_2,model_3,device, test_loader):\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                outputs_0 = model_0(images)\n",
    "                _, predicted_0 =torch.max(outputs_0.data, 1) \n",
    "\n",
    "                outputs_1 = model_1(images)\n",
    "                _, predicted_1 =torch.max(outputs_1.data, 1) \n",
    "\n",
    "                outputs_2 = model_2(images)\n",
    "                _, predicted_2 =torch.max(outputs_2.data, 1)\n",
    "\n",
    "                outputs_3 = model_3(images)\n",
    "                _, predicted_3 =torch.max(outputs_3.data, 1)\n",
    "\n",
    "                final_pred=predicted_1\n",
    "                size=final_pred.size()\n",
    "\n",
    "                for i in range(0,(size[0])):   \n",
    "                    a=0              \n",
    "                    if predicted_2[i].item()==0 and predicted_3[i].item()==0:\n",
    "\n",
    "                        if predicted_1[i].item()==1:\n",
    "                            final_pred[i]=1\n",
    "\n",
    "                        if predicted_1[i].item()==0:\n",
    "                            final_pred[i]=0\n",
    "                        a+=1\n",
    "\n",
    "                    if (predicted_0[i].item()==1 and predicted_1[i].item()==1):\n",
    "\n",
    "                        a+=1\n",
    "\n",
    "                        if predicted_3[i].item()==0:\n",
    "                            final_pred[i]=0                        \n",
    "                        if predicted_3[i].item()!=0:\n",
    "                            final_pred[i]=1\n",
    "                    if a==0:                   \n",
    "                        final_pred[i]=predicted_2[i]\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (final_pred == labels).sum().item()\n",
    "        print('Correct Prediction: {:d}  Total Images: {:d}'.format(correct, total))\n",
    "        print('Test Accuracy = {:f}'.format(correct / total))\n",
    "\n",
    "    def get_classification_report(truth, predict):  # create classification report for each class with scikit-learn library\n",
    "        print('Classification Report :\\n', classification_report(truth, predict))\n",
    "\n",
    "    def get_confusion_matrix(actuals, predictions):  # create confusion matrix for each class with scikit-learn library\n",
    "        matplotlib.rcdefaults()\n",
    "        print('Confusion matrix:\\n',confusion_matrix(actuals, predictions))\n",
    "        cf_matrix=confusion_matrix(actuals, predictions)\n",
    "        sns.heatmap(cf_matrix, annot=True,fmt='g', cmap='Blues')\n",
    "\n",
    "    def get_cohen_kappa(actuals, predictions):  # get cohen kapa score for   determine model performance\n",
    "        cps = cohen_kappa_score(actuals, predictions)\n",
    "        print('Kappa Score of this model:\\n', cps)\n",
    "\n",
    "    def test_class_probabilities(model, device, test_loader, which_class):\n",
    "        \n",
    "        truths = []\n",
    "        probabilities = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data).cuda().cpu()\n",
    "                prediction = output.argmax(dim=1, keepdim=True)\n",
    "                truths.extend(target.view_as(prediction) == which_class)\n",
    "                probabilities.extend(np.exp(output[:, which_class]))\n",
    "        return [i.item() for i in truths], [i.item() for i in probabilities]\n",
    "    def test_class_probabilities_el2(model_0,model_1,model_2,model_3, device, test_loader, which_class):\n",
    "    \n",
    "        truths = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                outputs_0 = model_0(data)\n",
    "                _, predicted_0 =torch.max(outputs_0.data, 1) \n",
    "\n",
    "                outputs_1 = model_1(data)\n",
    "                _, predicted_1 =torch.max(outputs_1.data, 1) \n",
    "\n",
    "                outputs_2 = model_2(data)\n",
    "                _, predicted_2 =torch.max(outputs_2.data, 1)\n",
    "\n",
    "                outputs_3 = model_3(data)\n",
    "                _, predicted_3 =torch.max(outputs_3.data, 1)\n",
    "\n",
    "                final_pred=predicted_1\n",
    "                out=outputs_1\n",
    "                size=final_pred.size()\n",
    "\n",
    "                for i in range(0,(size[0])):   \n",
    "                    a=0              \n",
    "                    if predicted_2[i].item()==0 and predicted_3[i].item()==0:\n",
    "\n",
    "                        if predicted_1[i].item()==1:\n",
    "                            #final_pred[i]=1\n",
    "                            out[i]=outputs_1[i]\n",
    "\n",
    "                        if predicted_1[i].item()==0:\n",
    "                            final_pred[i]=0\n",
    "                            out[i]=outputs_1[i]\n",
    "                        a+=1\n",
    "\n",
    "                    if (predicted_0[i].item()==1 and predicted_1[i].item()==1):\n",
    "\n",
    "                        a+=1\n",
    "\n",
    "                        if predicted_3[i].item()==0:\n",
    "                            #final_pred[i]=0\n",
    "                            out[i]=outputs_3[i]\n",
    "\n",
    "                        if predicted_3[i].item()!=0:\n",
    "                            #final_pred[i]=1\n",
    "                            out[i]=outputs_3[i]\n",
    "                    if a==0:                   \n",
    "                        #final_pred[i]=predicted_2[i]\n",
    "                        out[i]=outputs_2[i]\n",
    "                prediction = out.argmax(dim=1, keepdim=True)\n",
    "                truths.extend(target.view_as(prediction) == which_class)\n",
    "                probabilities.extend(np.exp(out.cuda().cpu()[:, which_class]))\n",
    "        return [i.item() for i in truths], [i.item() for i in probabilities]\n",
    "    \n",
    "    def get_roc_curves_el2(model_0,model_1,model_2,model_3, device, data):  # draw Roc curves and calculate auc score for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        actuals, class_probabilities = get_metric.test_class_probabilities_el2(model_0,model_1,model_2,model_3, device, data, 0)\n",
    "        fpr[0], tpr[0], _ = roc_curve(actuals, class_probabilities)\n",
    "        roc_auc[0] = roc_auc_score(actuals, class_probabilities)\n",
    "\n",
    "        actuals, class_probabilities = get_metric.test_class_probabilities_el2(model_0,model_1,model_2,model_3, device, data, 1)\n",
    "        fpr[1], tpr[1], _ = roc_curve(actuals, class_probabilities)\n",
    "        roc_auc[1] = roc_auc_score(actuals, class_probabilities)\n",
    "\n",
    "        print(\"Auc Score For Each Class: \", roc_auc)\n",
    "\n",
    "        matplotlib.rcdefaults()\n",
    "        plt.figure()\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "        for i, color in zip(range(2), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=1,\n",
    "            label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "            ''.format(i, roc_auc[i]))\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    def get_roc_curves(model, device, data):  # draw Roc curves and calculate auc score for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        actuals, class_probabilities = get_metric.test_class_probabilities(model, device, data, 0)\n",
    "        fpr[0], tpr[0], _ = roc_curve(actuals, class_probabilities)\n",
    "        roc_auc[0] = roc_auc_score(actuals, class_probabilities)\n",
    "\n",
    "        actuals, class_probabilities = get_metric.test_class_probabilities(model, device, data, 1)\n",
    "        fpr[1], tpr[1], _ = roc_curve(actuals, class_probabilities)\n",
    "        roc_auc[1] = roc_auc_score(actuals, class_probabilities)\n",
    "\n",
    "        actuals, class_probabilities = get_metric.test_class_probabilities(model, device, data, 2)\n",
    "        fpr[2], tpr[2], _ = roc_curve(actuals, class_probabilities)\n",
    "        roc_auc[2] = roc_auc_score(actuals, class_probabilities)\n",
    "\n",
    "        actuals, class_probabilities = get_metric.test_class_probabilities(model, device, data, 3)\n",
    "        fpr[3], tpr[3], _ = roc_curve(actuals, class_probabilities)\n",
    "        roc_auc[3] = roc_auc_score(actuals, class_probabilities)\n",
    "\n",
    "        print(\"Auc Score For Each Class: \", roc_auc)\n",
    "\n",
    "        matplotlib.rcdefaults()\n",
    "        plt.figure()\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "        for i, color in zip(range(2), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=1,\n",
    "                label='ROC curve of class {0} (area = {1:0.4f})'\n",
    "                    ''.format(i, roc_auc[i]))\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "densenet121\n",
      "-----------------\n",
      "F1 Score: 0.0 0.7230491655390167 0.7166212534059946\n",
      "Recall: 0.0 0.8366388308977035 0.6778350515463918 \n",
      "Precision: 0.0 0.6366163621922161 0.7601156069364162\n",
      "\n",
      "\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       596\n",
      "           1       0.64      0.84      0.72      1916\n",
      "           2       0.76      0.68      0.72      1940\n",
      "           3       0.68      0.82      0.74       946\n",
      "\n",
      "    accuracy                           0.69      5398\n",
      "   macro avg       0.52      0.58      0.55      5398\n",
      "weighted avg       0.62      0.69      0.64      5398\n",
      "\n",
      "Correct Prediction: 3698  Total Images: 5398\n",
      "Test Accuracy = 0.685069\n",
      "Kappa Score of this model:\n",
      " 0.5381638159105246\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[   0  585    1   10]\n",
      " [   0 1603  271   42]\n",
      " [   0  307 1315  318]\n",
      " [   0   23  143  780]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,roc_auc_score,roc_curve,precision_score,recall_score,confusion_matrix\n",
    "\n",
    "\n",
    "print('\\n'+'densenet121'+'\\n-----------------')  \n",
    "model_ft=torch.load('C:/Users/yeni/Desktop/TEKNOFEST/Mergen1-Teknofest/ertan/modelPerformance/densenet121/best_model_0.6979acc_20epochs.h5')\n",
    "phase='val'\n",
    "actuals, predictions = get_metric.test_label_predictions(model_ft, device, dataloaders[phase])\n",
    "f1=f1_score(predictions,actuals,average=None)\n",
    "recall=recall_score(actuals,predictions,average=None)\n",
    "precision=precision_score(actuals,predictions,average=None)\n",
    "print(f'F1 Score: {f1[0]} {f1[1]} {f1[2]}')\n",
    "print(f'Recall: {recall[0]} {recall[1]} {recall[2]} ')\n",
    "print(f'Precision: {precision[0]} {precision[1]} {precision[2]}')\n",
    "print('\\n')\n",
    "get_metric.get_classification_report(actuals, predictions)\n",
    "get_metric.test_model(model_ft,device,dataloaders[phase])\n",
    "get_metric.get_cohen_kappa(actuals, predictions)\n",
    "print('\\n')\n",
    "get_metric.get_confusion_matrix(actuals, predictions)\n",
    "print('\\n')\n",
    "#get_metric.get_roc_curves(model_ft, device,  dataloaders[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
