{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentasyon Mimarimiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydicom opencv-python-headless[app] nibabel matplotlib albumentations tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import jaccard_score\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "height,width = (256,256) \n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import albumentations as A\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "def mask_transform(mask, classes):\n",
    "    new_mask = np.zeros(mask.shape, dtype=np.uint8)\n",
    "    for i, label in enumerate(classes):\n",
    "        new_mask[mask == label] = i\n",
    "    return new_mask\n",
    "\n",
    "class LoadData(Dataset):\n",
    "    def __init__(self, images_path, masks_path, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.classes = classes\n",
    "        self.len = len(images_path)\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(height=height,width=width,p=1.0),\n",
    "            A.augmentations.transforms.CLAHE(clip_limit=(2.0,3.0), tile_grid_size=(8, 8), always_apply=False, p=0.5) \n",
    "        ])\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = pydicom.dcmread(self.images_path[idx])\n",
    "        monochrome=img.PhotometricInterpretation\n",
    "        img=img.pixel_array\n",
    "        mask = nib.load(self.masks_path[idx])\n",
    "        mask=mask.get_fdata()\n",
    "        \n",
    "        #3 kişinin farklı etiketleri için farklı bir kod yazıldı\n",
    "        if 'Sinem' in self.images_path[idx] :\n",
    "            if 'LCC' in self.images_path[idx]:\n",
    "                mask=mask[:,:,1]\n",
    "            elif 'LMLO' in self.images_path[idx]:\n",
    "                mask=mask[:,:,3]\n",
    "            elif 'RCC' in self.images_path[idx]:\n",
    "                mask=mask[:,:,0]\n",
    "            elif 'RMLO' in self.images_path[idx]:\n",
    "                mask=mask[:,:,2]\n",
    "\n",
    "        elif 'Ertan' in self.images_path[idx]:\n",
    "            mask=mask[:,:,0]\n",
    "            \n",
    "        elif 'Zeynep' in self.images_path[idx]:\n",
    "            if 'LCC' in self.images_path[idx]:\n",
    "                mask=mask[:,:,0]\n",
    "            elif 'LMLO' in self.images_path[idx]:\n",
    "                mask=mask[:,:,1]\n",
    "            elif 'RCC' in self.images_path[idx]:\n",
    "                mask=mask[:,:,2]\n",
    "            elif 'RMLO' in self.images_path[idx]:\n",
    "                mask=mask[:,:,3]\n",
    "\n",
    "        #maskeler tam oturmadığı için bu işlemler yapıldı\n",
    "        mask=np.flip(mask)\n",
    "        mask=np.rot90(mask)\n",
    "        mask=np.fliplr(mask)\n",
    "\n",
    "        if monochrome=='MONOCHROME1':\n",
    "            img=np.invert(img)\n",
    "\n",
    "        img = ((img - img.min()) * (1/(img.max() - img.min()) * 255)).astype('uint8')\n",
    "        \n",
    "        #img, mask = np.array(img), np.array(mask)\n",
    "        transformed = self.transform(image=img,mask=mask)\n",
    "        img = transformed['image']\n",
    "        mask = transformed['mask']\n",
    "        \n",
    "        #img = np.array([img,img,img])\n",
    "        #*****************************\n",
    "        #maskelerin her biri tek kanalda olacak şekilde ayrı ayrı oluşturuldu\n",
    "        mask_list=[]\n",
    "        temp_mask=mask.copy()\n",
    "        temp_mask[mask==1]=1\n",
    "        temp_mask[mask==2]=0\n",
    "        temp_mask[mask==3]=0\n",
    "        mask_list.append(temp_mask)\n",
    "\n",
    "        temp_mask=mask.copy()\n",
    "        temp_mask[mask==1]=0\n",
    "        temp_mask[mask==2]=1\n",
    "        temp_mask[mask==3]=0    #*1\n",
    "\n",
    "        mask_list.append(temp_mask)\n",
    "        \n",
    "        temp_mask=mask.copy()\n",
    "        temp_mask[mask==1]=0\n",
    "        temp_mask[mask==2]=0\n",
    "        temp_mask[mask==3]=1    #*0\n",
    "        mask_list.append(temp_mask)\n",
    "        \n",
    "        mask=np.array(mask_list)\n",
    "\n",
    "        img = np.array([img,img,img])\n",
    "\n",
    "        #********************************\n",
    "        img=np.moveaxis(img,0,-1)\n",
    "        mask=np.moveaxis(mask,0,-1)\n",
    "\n",
    "        img = self.to_tensor(img)\n",
    "\n",
    "        mask = self.to_tensor(mask)\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Veri kümesini oluşturun\n",
    "PATH=\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/data\"\n",
    "\n",
    "tr_images_path =sorted(glob.glob(f'{PATH}/images/[Ertan,Zeynep]*/*/*.dcm'))  # Görüntü yollarının listesi\n",
    "val_images_path =sorted(glob.glob(f'{PATH}/images/Sinem/*/*.dcm'))  # Görüntü yollarının listesi\n",
    "\n",
    "tr_masks_path=sorted(glob.glob(f'{PATH}/masks/[Ertan,Zeynep]*/*/*.nii.gz'))\n",
    "val_masks_path=sorted(glob.glob(f'{PATH}/masks/Sinem/*/*.nii.gz'))  # Maske yollarının listesi\n",
    "classes = [0, 1, 2]  # Orijinal etiket değerleri\n",
    "print(len(tr_images_path))\n",
    "print(len(tr_masks_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = LoadData(tr_images_path, tr_masks_path, classes)\n",
    "val_dataset = LoadData(val_images_path, val_masks_path, classes)\n",
    "\n",
    "# # Veri kümesini eğitim ve doğrulama alt kümelerine ayırma\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader oluşturma\n",
    "train_loader = DataLoader(tr_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs,masks=tr_dataset[341]\n",
    "\n",
    "\n",
    "# img=np.invert(imgs)\n",
    "#plt.imshow(imgs,cmap=\"gray\")\n",
    "\n",
    "# plt.imshow(np.transpose(imgs,(1,2,0)),cmap=\"inferno\")\n",
    "\n",
    "plt.imshow(np.transpose(masks,(1,2,0))[:,:,2],cmap=\"gray\")\n",
    "plt.imshow\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(masks,(1,2,0))[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.down_sample = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = self.double_conv(x)\n",
    "        down_out = self.down_sample(skip_out)\n",
    "        return (down_out, skip_out)\n",
    "\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlock, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, down_input, skip_input):\n",
    "        x = self.up_sample(down_input)\n",
    "        x = torch.cat([x, skip_input], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_classes=3, up_sample_mode='conv_transpose'):\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        # Downsampling Path\n",
    "        self.down_conv1 = DownBlock(3, 64)\n",
    "        self.down_conv2 = DownBlock(64, 128)\n",
    "        self.down_conv3 = DownBlock(128, 256)\n",
    "        self.down_conv4 = DownBlock(256, 512)\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024)\n",
    "        # Upsampling Path\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
    "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final Convolution\n",
    "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1_out = self.down_conv1(x)\n",
    "        x, skip2_out = self.down_conv2(x)\n",
    "        x, skip3_out = self.down_conv3(x)\n",
    "        x, skip4_out = self.down_conv4(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.up_conv4(x, skip4_out)\n",
    "        x = self.up_conv3(x, skip3_out)\n",
    "        x = self.up_conv2(x, skip2_out)\n",
    "        x = self.up_conv1(x, skip1_out)\n",
    "        x = self.conv_last(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Get UNet model\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def calculate_dice(y_true, y_pred):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    return (2 * intersection) / (np.sum(y_true) + np.sum(y_pred))\n",
    "\n",
    "def dice_loss(pred, target, epsilon=1e-7, use_sigmoid=True):\n",
    "    pred = pred.contiguous()\n",
    "    if use_sigmoid:\n",
    "        pred = torch.sigmoid(pred)\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + epsilon) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + epsilon)))\n",
    "    return loss.mean()\n",
    "\n",
    "def calculate_jaccard(y_true, y_pred):\n",
    "    # print(y_true.shape)\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred\n",
    "    return jaccard_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_image_mask(image, mask, pred_mask):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].imshow(mask.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")\n",
    "    ax[1].set_title(\"True Mask\")\n",
    "    ax[2].imshow(np.dstack((pred_mask[0], pred_mask[1], pred_mask[2])), cmap=\"gray\")\n",
    "    ax[2].set_title(\"Predicted Mask\")\n",
    "    print(pred_mask.shape)\n",
    "    print(np.unique(pred_mask[0,:,:]))\n",
    "    print(np.unique(mask.cpu().numpy()))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def visualize_image_mask(image, mask, pred_mask):\n",
    "    fig, ax = plt.subplots(1, 6, figsize=(15, 5))\n",
    "    ax[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].imshow(mask.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")\n",
    "    ax[1].set_title(\"True Mask\")\n",
    "    pred_mask1=np.array([pred_mask[0],pred_mask[0],pred_mask[0]])\n",
    "    pred_mask2=np.array([pred_mask[1],pred_mask[1],pred_mask[1]])\n",
    "    pred_mask3=np.array([pred_mask[2],pred_mask[2],pred_mask[2]])\n",
    "    ax[2].imshow(np.transpose(pred_mask1,(1,2,0)), cmap=\"gray\")\n",
    "    ax[2].set_title(\"Pektoral Predicted Mask\")\n",
    "    ax[3].imshow(np.transpose(pred_mask2,(1,2,0)), cmap=\"gray\")\n",
    "    ax[3].set_title(\"Meme Predicted Mask\")\n",
    "    ax[4].imshow(np.transpose(pred_mask3,(1,2,0)), cmap=\"gray\")\n",
    "    ax[4].set_title(\"Meme Ucu Predicted Mask\")\n",
    "    ax[5].imshow(np.transpose(pred_mask,(1,2,0)), cmap=\"gray\")\n",
    "    ax[5].set_title(\"All Predicted Mask\")\n",
    "\n",
    "    # print(pred_mask.shape)\n",
    "    # print(np.unique(pred_mask[0,:,:]))\n",
    "    # print(np.unique(mask.cpu().numpy()))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def validate_unet(model, val_loader, device, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    jaccard = 0.0\n",
    "    dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, masks) in enumerate(val_loader):\n",
    "            inputs, masks = inputs.cpu().to(device), masks.cpu().to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            # print(outputs.shape)\n",
    "            \n",
    "            output_np=outputs.detach().cpu()\n",
    "            output_np=np.array(output_np)\n",
    "            output_np=output_np-np.min(output_np)\n",
    "            output_np=output_np/np.max(output_np)\n",
    "            predicted_masks=np.around(output_np, decimals=0, out=None)\n",
    "            # _, predicted_masks = torch.max(outputs, 1)\n",
    "            # jaccard += calculate_jaccard(masks[0], predicted_masks[0])\n",
    "            dice += calculate_dice(masks[0], predicted_masks[0])\n",
    "\n",
    "            # İlk örnek için görselleştirme\n",
    "            \n",
    "            if i == 1:\n",
    "                visualize_image_mask(inputs[0], masks[0], predicted_masks[0])\n",
    "            if i == 2:\n",
    "                visualize_image_mask(inputs[0], masks[0], predicted_masks[0])\n",
    "            # if i == 3:\n",
    "            #     visualize_image_mask(inputs[0], masks[0], predicted_masks[0])\n",
    "            # if i == 4:\n",
    "            #     visualize_image_mask(inputs[0], masks[0], predicted_masks[0])\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    jaccard /= len(val_loader)\n",
    "    dice /= len(val_loader)\n",
    "    return val_loss, jaccard, dice\n",
    "\n",
    "def train_unet(epochs, device):\n",
    "    try:\n",
    "        os.mkdir(\"./checkpoints\")\n",
    "    except:\n",
    "        print(\"Checkpoints directory already exists\")\n",
    "\n",
    "    model = UNet()\n",
    "    model=model.to(device)\n",
    "    model.load_state_dict(torch.load(\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/checkpoints3/Murat3Kanal_8_ValLoss0.4248_diceScore0.9020.pt\"))\n",
    "    criterion = dice_loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    train_losses = []\n",
    "\n",
    "    best_dice = float('inf')*(-1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Eğitim\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, masks in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Doğrulama\n",
    "        val_loss, jaccard, dice = validate_unet(model, val_loader, device, criterion)\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Dice Score: {dice:.4f}\")\n",
    "\n",
    "        # En iyi modeli kaydetme\n",
    "        if dice > best_dice:\n",
    "            best_dice = dice\n",
    "            torch.save(model.state_dict(), f\"./checkpoints/{epoch}_ValLoss{val_loss:.4f}_diceScore{dice:.4f}.pt\")\n",
    "\n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model = train_unet(epochs=30, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "def visualize_test_mask(output_np,masks):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    print(output_np.shape)\n",
    "    axs[0].imshow(np.transpose(output_np[0],(1,2,0)),cmap=\"gray\")\n",
    "    axs[0].set_title(\"Predicted mask\")\n",
    "    axs[1].imshow(np.transpose(masks.cpu()[0],(1,2,0)))\n",
    "    axs[1].set_title(\"Ground truth mask\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_unet(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    jaccard = 0.0\n",
    "    dice = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, masks) in enumerate(test_loader):\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            output_np = outputs.detach().cpu().numpy()\n",
    "            output_np = output_np - np.min(output_np)\n",
    "            output_np = output_np / np.max(output_np)\n",
    "            predicted_masks = np.around(output_np, decimals=0, out=None)\n",
    "            \n",
    "            \n",
    "            visualize_image_mask(inputs[0], masks[0], predicted_masks[0])\n",
    "\n",
    "\n",
    "            dice += calculate_dice(masks[0], predicted_masks[0])\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    jaccard /= len(test_loader)\n",
    "    dice /= len(test_loader)\n",
    "    return test_loss, jaccard, dice\n",
    "\n",
    "def run_unet_test(model_path, test_dataset, device):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    model = UNet()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    criterion = dice_loss\n",
    "\n",
    "    test_loss, jaccard, dice = test_unet(model, test_loader, device, criterion)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Dice Score: {dice:.4f}\")\n",
    "\n",
    "run_unet_test(f\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/checkpoints3/Murat3Kanal_8_ValLoss0.4248_diceScore0.9020.pt\",val_dataset,device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
