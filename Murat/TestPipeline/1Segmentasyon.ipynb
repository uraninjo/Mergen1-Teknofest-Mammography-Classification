{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydicom opencv-python-headless[app] nibabel matplotlib albumentations tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import jaccard_score\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "height,width = (256,256) \n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.down_sample = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = self.double_conv(x)\n",
    "        down_out = self.down_sample(skip_out)\n",
    "        return (down_out, skip_out)\n",
    "\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlock, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, down_input, skip_input):\n",
    "        x = self.up_sample(down_input)\n",
    "        x = torch.cat([x, skip_input], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_classes=3, up_sample_mode='conv_transpose'):\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        # Downsampling Path\n",
    "        self.down_conv1 = DownBlock(3, 64)\n",
    "        self.down_conv2 = DownBlock(64, 128)\n",
    "        self.down_conv3 = DownBlock(128, 256)\n",
    "        self.down_conv4 = DownBlock(256, 512)\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024)\n",
    "        # Upsampling Path\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
    "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final Convolution\n",
    "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1_out = self.down_conv1(x)\n",
    "        x, skip2_out = self.down_conv2(x)\n",
    "        x, skip3_out = self.down_conv3(x)\n",
    "        x, skip4_out = self.down_conv4(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.up_conv4(x, skip4_out)\n",
    "        x = self.up_conv3(x, skip3_out)\n",
    "        x = self.up_conv2(x, skip2_out)\n",
    "        x = self.up_conv1(x, skip1_out)\n",
    "        x = self.conv_last(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Get UNet model\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pydicom  # pydicom kütüphanesi DICOM formatındaki tıbbi görüntüleri okumak için kullanılır.\n",
    "import nibabel as nib  # nibabel kütüphanesi NIfTI formatındaki tıbbi görüntüleri okumak için kullanılır.\n",
    "import numpy as np  # numpy kütüphanesi matematiksel işlemler ve veri işleme için kullanılır.\n",
    "import matplotlib.pyplot as plt  # matplotlib kütüphanesi verileri görselleştirmek için kullanılır.\n",
    "from skimage import measure  # skimage kütüphanesi görüntü işleme ve analizi için kullanılır.\n",
    "\n",
    "class Temp:\n",
    "    _temp_img=None  # Sınıf içinde kullanılacak geçici değişkenler için Temp sınıfı oluşturuldu.\n",
    "    _temp_mock=None\n",
    "    rel_vals=None\n",
    "\n",
    "def get_largest_mask(mask):\n",
    "    \"\"\"\n",
    "    Fonksiyon, en büyük maskeyi elde etmek için girdi olarak aldığı maske görüntüsünün etiketlerini ve \n",
    "    özelliklerini hesaplar ve sonra maske üzerindeki bölgeleri büyüklüklerine göre sıralar. \n",
    "    Son olarak, en büyük bölgeyi elde etmek için diğer tüm bölgeleri sıfırlar.\n",
    "    \"\"\"\n",
    "    labels_mask = measure.label(mask)  # maske görüntüsünün etiketlerini hesaplar\n",
    "    regions = measure.regionprops(labels_mask)  # etiketli bölgelerin özelliklerini hesaplar\n",
    "    regions.sort(key=lambda x: x.area, reverse=True)  # bölgeleri alanlarına göre sıralar\n",
    "    if len(regions) > 1:  # en büyük bölgenin dışında diğer bölgeleri sıfırlar\n",
    "        for rg in regions[1:]:\n",
    "            labels_mask[rg.coords[:,0], rg.coords[:,1]] = 0\n",
    "    labels_mask[labels_mask!=0] = 1  # maske görüntüsünde kalan tüm bölgeleri birleştirir\n",
    "    mask = labels_mask\n",
    "    return mask \n",
    "\n",
    "def crop_and_mask_dcm_image(masks, real_img):\n",
    "    \"\"\"\n",
    "    Fonksiyon, DICOM görüntüsünü ve onun maske görüntüsünü alır. Görüntü boyutları eşit olmadığı durumlarda\n",
    "    maske boyutunu gerçek görüntüye uyarlar. Daha sonra en büyük maskeyi elde etmek için 'get_largest_mask' \n",
    "    fonksiyonunu kullanır. Son olarak, görüntüyü maskeyle çarparak maske dışındaki alanları siyah renge döker.\n",
    "    \"\"\"\n",
    "    real_w,real_h=real_img.shape  # gerçek görüntünün boyutlarını hesaplar\n",
    "    mock_masks=cv2.resize(masks,(real_h,real_w), interpolation=cv2.INTER_AREA)  # maske boyutunu gerçek görüntüye uyarlama işlemi yapar\n",
    "    \n",
    "    Temp._temp_mock=mock_masks\n",
    "    mock_masks=get_largest_mask(mock_masks)  # en büyük maskeyi elde eder\n",
    "    rslt = mock_masks* real_img  # maske görüntüsü ve gerçek görüntüyü çarparak sonucu elde eder\n",
    "\n",
    "    rows, cols = np.where(mock_masks!= 0)  # maske görüntüsünde sıfırdan farklı değerlerin satır ve sütun indislerini elde eder\n",
    "    y1, x1 = np.min(rows), np.min(cols)  # satır ve sütun indislerinin en küçük değerlerini hesaplar\n",
    "    y2, x2 = np.max(rows), np.max(cols)  # satır ve sütun indislerinin en büyük değerlerini hesaplar\n",
    "\n",
    "    # Görüntüyü bounding box ile kırp\n",
    "    result = rslt[y1:y2+1, x1:x2+1]  # bounding box işlemi yaparak sonucu elde eder\n",
    "\n",
    "    # Maske dışındaki alanı siyah renge dök\n",
    "    result[mock_masks[y1:y2+1, x1:x2+1] == 0] = 0  # maske dışındaki alanları siyah renge dökme işlemi yapar\n",
    "\n",
    "    Temp._temp_img=result\n",
    "\n",
    "    return rslt, result  # sonuçları döndürür\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "PATH=\"/home/uraninjo/Documents/VEri\"# TODO: Change this\n",
    "#PATH=\"/media/uraninjo/7D72-19E7/teknofest\"\n",
    "test_images_path=sorted(glob.glob(f'{PATH}/*/*.dcm'))  # Görüntü yollarının listesi\n",
    "\n",
    "classes = [0, 1, 2]  # Orijinal etiket değerleri\n",
    "print(len(test_images_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.load_state_dict(torch.load(\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/checkpoints3/Kullanılacaklar/Segmentasyon_Zeynep2Kanal_6_ValLoss0.5481_diceScore0.9260.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "            A.Resize(height=256,width=256,p=1.0),\n",
    "            A.augmentations.transforms.CLAHE(clip_limit=(2.0,3.0), tile_grid_size=(8, 8), always_apply=False, p=0.5) \n",
    "        ])\n",
    "to_tensor = ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "\n",
    "# SegmentationOutput klasörünün oluşturulması\n",
    "try:\n",
    "    os.mkdir(f\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/TestPipeline/SegmentasyonOutput\")#{__hno}\n",
    "    # TODO: Change this\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tüm test görüntülerinin üzerinde döngüye girilmesi\n",
    "    for i in range(len(test_images_path)):\n",
    "        try:\n",
    "            # Görüntü türü ve hasta numarasının elde edilmesi\n",
    "            __tür=test_images_path[i].split(\"/\")[-1].split(\".\")[0]\n",
    "            __hno=test_images_path[i].split(\"/\")[-2]\n",
    "            \n",
    "            # Görüntünün okunması ve piksel dizisine dönüştürülmesi\n",
    "            img = pydicom.dcmread(test_images_path[i], force=True)\n",
    "            monochrome=img.PhotometricInterpretation\n",
    "            img=img.pixel_array\n",
    "            if monochrome=='MONOCHROME1':\n",
    "                img=np.invert(img)\n",
    "            img = ((img - img.min()) * (1/(img.max() - img.min()) * 255)).astype('uint8')\n",
    "            \n",
    "            # Görüntünün boyutunun yeniden boyutlandırılması ve tensor formatına dönüştürülmesi\n",
    "            transformed = transform(image=img)\n",
    "            img_t = transformed[\"image\"]\n",
    "            img_t = np.array([img_t,img_t,img_t])\n",
    "            img_t=np.moveaxis(img_t,0,-1)\n",
    "            img_t = to_tensor(img_t)\n",
    "            img_t = torch.unsqueeze(img_t, 0)\n",
    "            inputs = img_t.to(device)\n",
    "            \n",
    "            # Görüntü segmentasyonunun yapılması\n",
    "            outputs = model(inputs)\n",
    "            output_np = outputs.detach().cpu().numpy()\n",
    "            output_np = output_np - np.min(output_np)\n",
    "            output_np = output_np / np.max(output_np)\n",
    "            predicted_masks = np.around(output_np, decimals=0, out=None)\n",
    "            mask_1 = predicted_masks[0][1,:,:]\n",
    "            mask_2 = predicted_masks[0][2,:,:]\n",
    "            combined_mask = np.maximum(mask_1, mask_2)\n",
    "\n",
    "            # Görüntü kırpma ve maskeleme işlemlerinin yapılması\n",
    "            rslt, result = crop_and_mask_dcm_image(combined_mask,img)#inputs[0][0,:,:].cpu().numpy()\n",
    "            result_resize = cv2.resize(result.astype('uint8'),(250,500))\n",
    "            \n",
    "            # Hasta klasörünün oluşturulması\n",
    "            try:\n",
    "                os.mkdir(f\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/TestPipeline/SegmentasyonOutput/{__hno}\")#{__hno}\n",
    "                # TODO: Change this\n",
    "            except:\n",
    "                pass\n",
    "                # print(\"FoldAttributeError: 'FileDataset' object has no attribute 'PhotometricInterpretation'er already exists\")\n",
    "                \n",
    "            # Sonuç görüntüsünün kaydedilmesi\n",
    "            plt.imsave(f\"/home/uraninjo/Desktop/Çalışmalar/Mergen1-Teknofest/Murat/TestPipeline/SegmentasyonOutput/{__hno}/{__tür}.png\",result_resize,cmap=\"gray\")\n",
    "                        # Ekrana bilgi yazdırılması\n",
    "            print(f\"Dosya oluşturuldu sıra:{i}-{__tür}-{__hno}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Hatalı\", test_images_path[i])\n",
    "            continue\n",
    "            \n",
    "t2=time.time()\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
